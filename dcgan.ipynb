{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to resize images and normalize them\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),  # Resize images to 64x64\n",
    "    transforms.CenterCrop(64),  # Crop the image to 64x64\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('generated_images', exist_ok=True)\n",
    "\n",
    "# Load your dataset (replace 'crack_images' with the actual directory of your dataset)\n",
    "dataset = datasets.ImageFolder(root=\"crack_dataset\", transform=transform)\n",
    "\n",
    "# Create DataLoader for training\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is Z, going into a fully connected layer\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # State size. (512) x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # State size. (256) x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # State size. (128) x 16 x 16\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # State size. (64) x 32 x 32\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()  # Final output layer, scale the output to [-1, 1] for image pixel values\n",
    "            # State size. (3) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DCGAN Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. (64) x 32 x 32\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. (128) x 16 x 16\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. (256) x 8 x 8\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. (512) x 4 x 4\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()  # Output probability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/25], Step [0/8], D Loss: 1.2859406471252441, G Loss: 2.8667752742767334\n",
      "Epoch [1/25], Step [0/8], D Loss: 0.37672871351242065, G Loss: 7.609241008758545\n",
      "Epoch [2/25], Step [0/8], D Loss: 0.14128120243549347, G Loss: 10.725042343139648\n",
      "Epoch [3/25], Step [0/8], D Loss: 0.11249386519193649, G Loss: 15.652231216430664\n",
      "Epoch [4/25], Step [0/8], D Loss: 2.3848040103912354, G Loss: 16.215341567993164\n",
      "Epoch [5/25], Step [0/8], D Loss: 0.038313861936330795, G Loss: 8.704952239990234\n",
      "Epoch [6/25], Step [0/8], D Loss: 0.05956666171550751, G Loss: 9.552754402160645\n",
      "Epoch [7/25], Step [0/8], D Loss: 0.10948708653450012, G Loss: 11.875425338745117\n",
      "Epoch [8/25], Step [0/8], D Loss: 0.7137685418128967, G Loss: 14.383546829223633\n",
      "Epoch [9/25], Step [0/8], D Loss: 0.08276932686567307, G Loss: 6.741725921630859\n",
      "Epoch [10/25], Step [0/8], D Loss: 0.4594111442565918, G Loss: 9.40829849243164\n",
      "Epoch [11/25], Step [0/8], D Loss: 1.150230050086975, G Loss: 4.85722017288208\n",
      "Epoch [12/25], Step [0/8], D Loss: 0.16462717950344086, G Loss: 5.361304759979248\n",
      "Epoch [13/25], Step [0/8], D Loss: 0.383450448513031, G Loss: 6.2671990394592285\n",
      "Epoch [14/25], Step [0/8], D Loss: 0.2970547378063202, G Loss: 4.428678512573242\n",
      "Epoch [15/25], Step [0/8], D Loss: 0.18131165206432343, G Loss: 5.074960708618164\n",
      "Epoch [16/25], Step [0/8], D Loss: 0.19222065806388855, G Loss: 5.8242292404174805\n",
      "Epoch [17/25], Step [0/8], D Loss: 0.8663920164108276, G Loss: 1.8011800050735474\n",
      "Epoch [18/25], Step [0/8], D Loss: 0.29141324758529663, G Loss: 3.6008870601654053\n",
      "Epoch [19/25], Step [0/8], D Loss: 0.16358309984207153, G Loss: 4.235153675079346\n",
      "Epoch [20/25], Step [0/8], D Loss: 0.7169798016548157, G Loss: 1.8575927019119263\n",
      "Epoch [21/25], Step [0/8], D Loss: 0.16402089595794678, G Loss: 4.352872371673584\n",
      "Epoch [22/25], Step [0/8], D Loss: 0.2542009949684143, G Loss: 5.8915557861328125\n",
      "Epoch [23/25], Step [0/8], D Loss: 0.2187708020210266, G Loss: 5.002264499664307\n",
      "Epoch [24/25], Step [0/8], D Loss: 0.16031242907047272, G Loss: 5.879134178161621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, _) in enumerate(dataloader):\n",
    "        # Create labels for the real and fake images\n",
    "        real_images = data\n",
    "        batch_size = real_images.size(0)\n",
    "        labels_real = torch.ones(batch_size, 1)\n",
    "        labels_fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "      # Train the Discriminator\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Get discriminator's output for real images\n",
    "        output_real = discriminator(real_images)\n",
    "        output_real = output_real.view(-1)  # Squeeze to shape [batch_size]\n",
    "        labels_real = labels_real.view(-1)  # Ensure labels are also [batch_size]\n",
    "        loss_d_real = criterion(output_real, labels_real)\n",
    "\n",
    "        # Create fake images\n",
    "        noise = torch.randn(batch_size, 100, 1, 1)\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "        # Get discriminator's output for fake images\n",
    "        output_fake = discriminator(fake_images.detach())\n",
    "        output_fake = output_fake.view(-1)  # Squeeze to shape [batch_size]\n",
    "        labels_fake = labels_fake.view(-1)  # Ensure labels are also [batch_size]\n",
    "        loss_d_fake = criterion(output_fake, labels_fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Train the Generator\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Get discriminator's output for fake images\n",
    "        output_fake = discriminator(fake_images)\n",
    "        output_fake = output_fake.view(-1)  # Squeeze to shape [batch_size]\n",
    "\n",
    "        # Calculate generator loss\n",
    "        loss_g = criterion(output_fake, labels_real)  # labels_real is already [batch_size]\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "\n",
    "        # Print the losses\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(dataloader)}], \"\n",
    "                  f\"D Loss: {loss_d.item()}, G Loss: {loss_g.item()}\")\n",
    "\n",
    "    # Save generated images every epoch\n",
    "    save_image(fake_images.data, f\"generated_images/epoch_{epoch}.png\", normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained models\n",
    "torch.save(generator.state_dict(), \"generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crack_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
